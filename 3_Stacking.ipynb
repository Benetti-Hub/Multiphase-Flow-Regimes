{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacked Classifier - Optuna\n",
    "\n",
    "This notebook will perfrom AutoML with Bayesian inference to determine the best possible estimator for classification problem.\n",
    "The best estimaotrs are also aggregated using a meta learner in order to generate a stacked classifier. Given the time complexity\n",
    "of the problem at hand, the stacked classifier class can perfrom prarallel trials on multiple machines:\n",
    "$\\textit{This requires to setup a proper connection to a database, see the function \"database_location\" of the StackedClassifier class}$\n",
    "\n",
    "The StackedClassifier class is very flexibile, and can accomodate any type of \"in-fold\" operation, such as the normalization of the input features, the oversampling of the minority classes and so on. To check the parameters that are explored by Optuna trial, check \"./script/model_optuna.py\".\n",
    "\n",
    "Addiontally, this script is also used to perfrom some form of model evaluation, by plotting the results against unseen experimental conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For development\n",
    "#Reload the library when a change is detected in one of the imported libraries\n",
    "%load_ext autoreload \n",
    "%autoreload 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2021-09-02 08:48:43.972340: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2021-09-02 08:48:43.972377: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "from script import stacking\n",
    "from script import utils\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.combine import SMOTETomek\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>P</th>\n",
       "      <th>T</th>\n",
       "      <th>DenL</th>\n",
       "      <th>DenG</th>\n",
       "      <th>VisL</th>\n",
       "      <th>VisG</th>\n",
       "      <th>ST</th>\n",
       "      <th>ID</th>\n",
       "      <th>Roughness</th>\n",
       "      <th>Ang</th>\n",
       "      <th>Vsl</th>\n",
       "      <th>Vsg</th>\n",
       "      <th>Flow_label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>879.8</td>\n",
       "      <td>1.30</td>\n",
       "      <td>0.483</td>\n",
       "      <td>0.000018</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.0508</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.010940</td>\n",
       "      <td>8.033540</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>1.800000</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-90.0</td>\n",
       "      <td>0.986322</td>\n",
       "      <td>0.021483</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0510</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.100000</td>\n",
       "      <td>16.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100.0</td>\n",
       "      <td>25.0</td>\n",
       "      <td>1000.0</td>\n",
       "      <td>1.18</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.000015</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.0250</td>\n",
       "      <td>0.0</td>\n",
       "      <td>90.0</td>\n",
       "      <td>1.573620</td>\n",
       "      <td>0.967343</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       P     T    DenL  DenG   VisL      VisG    ST      ID  Roughness   Ang  \\\n",
       "0  100.0  25.0   879.8  1.30  0.483  0.000018  0.03  0.0508        0.0   0.0   \n",
       "1  100.0  25.0  1000.0  1.18  0.001  0.000015  0.07  0.0510        0.0  -1.0   \n",
       "2  100.0  25.0  1000.0  1.18  0.001  0.000015  0.07  0.0250        0.0 -90.0   \n",
       "3  100.0  25.0  1000.0  1.18  0.001  0.000015  0.07  0.0510        0.0   0.0   \n",
       "4  100.0  25.0  1000.0  1.18  0.001  0.000015  0.07  0.0250        0.0  90.0   \n",
       "\n",
       "        Vsl        Vsg  Flow_label  \n",
       "0  0.010940   8.033540         3.0  \n",
       "1  1.800000   0.025000         1.0  \n",
       "2  0.986322   0.021483         2.0  \n",
       "3  0.100000  16.000000         0.0  \n",
       "4  1.573620   0.967343         2.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('Data/Train_bronze.csv') #Train Dataset\n",
    "df = utils.bronze_to_silver(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kept_columns = ['Ang', 'FrL', 'FrG', 'X_LM_2', 'Eo', 'Flow_label'] #Kept Columns (problem specific)\n",
    "algos = [\"LightGBM\"] #Algos to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Stacked Classifier object\n",
    "\n",
    "An object of the StackingClassifier class is initiatied, it contains info about the models to explore, the column to keep, and the in-fold operations to perform during the optimization of the models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = stacking.StackedClassifier(base_algos=algos, balance_method=SMOTE(),\n",
    "                                         database_loc='local', cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.make_clean('results/info/') #REMOVE THE FOLDER SPEFICIED \n",
    "sc.make_clean('results/db/') #REMOVE THE FOLDER SPECIFIED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Base Models\n",
    "\n",
    "We can now optimize the models using the train dataframe. Feed a dataframe (where the last column is the target) and a number of trials to explore for each algo. Go run a marathon, when you are back if you are lucky the AutoML process will be compleated. You can interrupt this process at any time, no information will be lost. If you already have a trained base, just set the second argument to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-02 08:58:33,037]\u001b[0m Using an existing study with name 'LightGBM optimization' instead of creating a new one.\u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:34,863]\u001b[0m Trial 40 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:38,759]\u001b[0m Trial 41 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:41,199]\u001b[0m Trial 42 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:44,005]\u001b[0m Trial 43 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:46,511]\u001b[0m Trial 44 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:47,896]\u001b[0m Trial 45 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:50,873]\u001b[0m Trial 46 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:52,250]\u001b[0m Trial 47 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:53,453]\u001b[0m Trial 48 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:55,348]\u001b[0m Trial 49 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:56,565]\u001b[0m Trial 50 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:58:59,070]\u001b[0m Trial 51 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:01,490]\u001b[0m Trial 52 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:03,462]\u001b[0m Trial 53 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:05,142]\u001b[0m Trial 54 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:06,853]\u001b[0m Trial 55 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:08,827]\u001b[0m Trial 56 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:09,818]\u001b[0m Trial 57 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:12,165]\u001b[0m Trial 58 pruned. \u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:14,340]\u001b[0m Trial 59 pruned. \u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sc.optimize_base(df, 20, kept=kept_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the Meta-Learner\n",
    "\n",
    "Optimize the meta learner using the exact same approach as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2021-09-02 08:59:50,068]\u001b[0m A new study created in memory with name: LightGBM meta optimization\u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:50,516]\u001b[0m Trial 0 finished with value: 0.9457292103503047 and parameters: {'num_leaves': 8, 'learning_rate': 0.02, 'n_estimators': 66, 'subsample': 1.0, 'subsample_freq': 1, 'reg_alpha': 0.0500060150217337, 'reg_lambda': 0.10452603478326139, 'colsample_bytree': 0.4, 'min_child_samples': 166}. Best is trial 0 with value: 0.9457292103503047.\u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:51,730]\u001b[0m Trial 1 finished with value: 0.9457292103503047 and parameters: {'num_leaves': 394, 'learning_rate': 0.017, 'n_estimators': 156, 'subsample': 0.5, 'subsample_freq': 100, 'reg_alpha': 0.000587754234691692, 'reg_lambda': 0.0001288393909671829, 'colsample_bytree': 0.8, 'min_child_samples': 153}. Best is trial 0 with value: 0.9457292103503047.\u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:53,234]\u001b[0m Trial 2 finished with value: 0.9457292103503047 and parameters: {'num_leaves': 391, 'learning_rate': 0.02, 'n_estimators': 223, 'subsample': 0.8, 'subsample_freq': 10, 'reg_alpha': 7.969597789367559, 'reg_lambda': 1.952710632570678, 'colsample_bytree': 1.0, 'min_child_samples': 97}. Best is trial 0 with value: 0.9457292103503047.\u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:53,995]\u001b[0m Trial 3 finished with value: 0.9457292103503047 and parameters: {'num_leaves': 270, 'learning_rate': 0.1, 'n_estimators': 99, 'subsample': 1.0, 'subsample_freq': 0, 'reg_alpha': 3.10880615563879e-07, 'reg_lambda': 0.0014912049755235767, 'colsample_bytree': 0.7, 'min_child_samples': 48}. Best is trial 0 with value: 0.9457292103503047.\u001b[0m\n",
      "\u001b[32m[I 2021-09-02 08:59:55,698]\u001b[0m Trial 4 finished with value: 0.9457292103503047 and parameters: {'num_leaves': 209, 'learning_rate': 0.006, 'n_estimators': 265, 'subsample': 0.6, 'subsample_freq': 100, 'reg_alpha': 0.4143867173827114, 'reg_lambda': 0.596559153012536, 'colsample_bytree': 0.4, 'min_child_samples': 246}. Best is trial 0 with value: 0.9457292103503047.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "sc.train_meta('LightGBM', n_trials=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and Save the Base Models\n",
    "\n",
    "We are now ready to train and save the best models found using all the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Data/Dataset.csv')\n",
    "df_train = utils.bronze_to_gold(df_train, balance_method=SMOTE(), kept_columns=kept_columns)\n",
    "X_train, y_train = df_train.iloc[:,:-1].values, df_train.iloc[:,[-1]].values.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "first_it = True\n",
    "if first_it:\n",
    "    #Train\n",
    "    sc.train_base(X_train, y_train, remove_old=True)\n",
    "else:\n",
    "    #Load\n",
    "    sc.load_base()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_df(df_bronze, n_class=6):\n",
    "    \n",
    "    '''\n",
    "    Simple function to check the performance of the single\n",
    "    estimators on the test data\n",
    "    '''\n",
    "    \n",
    "    df_gold = utils.bronze_to_gold(df_bronze, kept_columns=kept_columns)\n",
    "    \n",
    "    X_test = df_gold.iloc[:,:-1].values    \n",
    "    y_test = df_gold.iloc[:,[-1]].values.ravel()\n",
    "    \n",
    "    for algo, model in sc.get_base().items():\n",
    "        print(algo)\n",
    "        y_pred = model.predict(X_test)\n",
    "        y_pred = y_pred if y_pred.ndim==1 else y_pred.argmax(1)\n",
    "        \n",
    "        if n_class==4:\n",
    "            y_pred = np.where(y_pred==5, 1, y_pred)\n",
    "            y_pred = np.where(y_pred==4, 3, y_pred)\n",
    "        \n",
    "        utils.print_performance(y_test, y_pred)\n",
    "    \n",
    "    print('StackedClassifier')\n",
    "    y_pred = sc.predict(X_test)\n",
    "    if n_class==4:\n",
    "        y_pred = np.where(y_pred==5, 1, y_pred)\n",
    "        y_pred = np.where(y_pred==4, 3, y_pred)\n",
    "    \n",
    "    utils.print_performance(y_test, y_pred)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We can now use the single estimators to check the performance on the test set. This is just a final check, and if everything went smoothly, the best single estimator will be the best performing one."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df_test_same = pd.read_csv('Data/Test_bronze.csv')\n",
    "predict_df(df_test_same)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extra Evaluation\n",
    "\n",
    "The test set was taken with respect to experimental coditions equal to the ones of the test set. It is clear that this does not really refect a real case scenario, where the experimental conditions may differ from the ones of the training dataset. To this end, the dataset from the study of Mexico numba one is used to test the pipeline with respect to previously unseen experimental conditions. It should be underlined that this dataset only contains 4 types of regimes, so the output of the pipeline have to be modified accordingly (by aggregating dispersed and stratified regimes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LightGBM\n",
      "Mean Accuracy:  0.7426687883609912\n",
      "Mean F1 score:  0.710848370944936\n",
      "\n",
      "\n",
      "Single class Accuracy:  [0.59647189 0.62676056 0.85403151 0.68798956]\n",
      "Single class F1 score:  [0.68308081 0.62620932 0.79132675 0.7427766 ]\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.60      0.68       907\n",
      "           1       0.63      0.63      0.63       568\n",
      "           2       0.74      0.85      0.79      2158\n",
      "           3       0.81      0.69      0.74       766\n",
      "\n",
      "    accuracy                           0.74      4399\n",
      "   macro avg       0.74      0.69      0.71      4399\n",
      "weighted avg       0.75      0.74      0.74      4399\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 541   39  282   45]\n",
      " [  15  356  194    3]\n",
      " [  70  167 1843   78]\n",
      " [  51    7  181  527]]\n",
      "StackedClassifier\n",
      "Mean Accuracy:  0.7426687883609912\n",
      "Mean F1 score:  0.710848370944936\n",
      "\n",
      "\n",
      "Single class Accuracy:  [0.59647189 0.62676056 0.85403151 0.68798956]\n",
      "Single class F1 score:  [0.68308081 0.62620932 0.79132675 0.7427766 ]\n",
      "\n",
      "Classification Report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.80      0.60      0.68       907\n",
      "           1       0.63      0.63      0.63       568\n",
      "           2       0.74      0.85      0.79      2158\n",
      "           3       0.81      0.69      0.74       766\n",
      "\n",
      "    accuracy                           0.74      4399\n",
      "   macro avg       0.74      0.69      0.71      4399\n",
      "weighted avg       0.75      0.74      0.74      4399\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      " [[ 541   39  282   45]\n",
      " [  15  356  194    3]\n",
      " [  70  167 1843   78]\n",
      " [  51    7  181  527]]\n"
     ]
    }
   ],
   "source": [
    "df_test_secret = pd.read_csv('Data/Secret/Test_secret.csv')\n",
    "df_secret_ID = df_test_secret.loc[(df_test_secret[\"ID\"]<0.05) & (df_test_secret[\"ID\"]>0.01) & (df_test_secret['P']<300)]\n",
    "predict_df(df_test_secret, n_class=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Visualization\n",
    "\n",
    "It is now time to present the results in a respectable manner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "algos = [\"LightGBM\", \"RandomForest\", \"MLP\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bar_plot_studies(df_test, algos=algos, n_class=6):\n",
    "    \n",
    "    '''\n",
    "    Another simple function to show the accuracy on the single studies\n",
    "    '''\n",
    "    \n",
    "    author_list = list(df_test['Author'].value_counts().index)\n",
    "    model_info = pd.DataFrame(index=author_list, columns=algos)\n",
    "\n",
    "    for author in author_list:\n",
    "\n",
    "        di = {}\n",
    "\n",
    "        df_author = df_test.loc[(df_test['Author']==author)]\n",
    "        df_author = utils.bronze_to_gold(df_author, kept_columns=kept_columns)\n",
    "        X_test = df_author.iloc[:,:-1].values\n",
    "        y_test = df_author.iloc[:,[-1]].values.ravel()\n",
    "\n",
    "        for algo, model in sc.get_base().items():\n",
    "            y_pred = model.predict(X_test)\n",
    "            y_pred = y_pred if y_pred.ndim==1 else y_pred.argmax(1)\n",
    "            if n_class==4:\n",
    "                y_pred = np.where(y_pred==5, 1, y_pred)\n",
    "                y_pred = np.where(y_pred==4, 3, y_pred)\n",
    "            \n",
    "            di[algo] = accuracy_score(y_test, y_pred)\n",
    "        \n",
    "        y_pred = sc.predict(X_test)\n",
    "        if n_class==4:\n",
    "            y_pred = np.where(y_pred==5, 1, y_pred)\n",
    "            y_pred = np.where(y_pred==4, 3, y_pred)\n",
    "        \n",
    "        di[\"StackedEnsamble\"] = accuracy_score(y_test, y_pred)\n",
    "        model_info.loc[author] = di \n",
    "\n",
    "    model_info = model_info.sort_index()\n",
    "    fig = go.Figure(data=[go.Bar(name=algo, y=model_info[algo], x=model_info.index) for algo in algos])\n",
    "    fig.update_yaxes(title=\"Accuracy\")\n",
    "    fig.update_xaxes(title=\"Independent Study\", tickangle=45)\n",
    "    fig.update_layout(barmode='group', title=\"Prediction Accuracy on different studies\")\n",
    "    \n",
    "    fig.write_image(f\"Plots/Others/prediction_accuracy_{len(author_list)}.png\", scale=2)\n",
    "    \n",
    "    fig.show()\n",
    "    \n",
    "    return model_info"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "_ = bar_plot_studies(df_test_same)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mi = bar_plot_studies(df_test_secret, n_class=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = pd.DataFrame({\"mean\" : mi.mean(), \"std\" : mi.std()})\n",
    "\n",
    "fig = go.Figure(data=[go.Bar(name=\"Averages\", \n",
    "                             y=averages[\"mean\"], \n",
    "                             x=averages.index,\n",
    "                             error_y=dict(type='data', array=averages['std']))])\n",
    "\n",
    "fig.update_layout(barmode='group')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "infos = pd.read_parquet('results/info/logs.parquet')\n",
    "algos = [\"LightGBM\", \"RandomForest\", \"MLP\", \"TabNet\", \"XGBoost\"]\n",
    "infos = infos.loc[infos[\"Algo\"].isin(algos)]\n",
    "\n",
    "bar = infos.loc[infos.groupby(['Algo'])['Accuracy'].idxmax()].sort_values(by='Accuracy')\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Bar(\n",
    "    name='Accuracy',\n",
    "    x=bar['Algo'], y=round(bar['Accuracy'], 3),\n",
    "    error_y=dict(type='data', array=3*round(bar['Accuracy_std'], 3)),\n",
    "    text=round(bar['Accuracy'], 3),\n",
    "    textposition=['none', 'none', 'none', 'none', 'none', 'none']\n",
    "))\n",
    "fig.add_trace(go.Bar(\n",
    "    name='F1_score',\n",
    "    x=bar['Algo'], y=round(bar['F1_score'], 3),\n",
    "    error_y=dict(type='data', array=3*round(bar['F1_score_std'], 3)),\n",
    "    text=round(bar['F1_score'], 3),\n",
    "    textposition=['none', 'none', 'none', 'none', 'none']\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=450,\n",
    "    title=\"Cross Validation Results\",\n",
    "    xaxis_title=\"Machine learning model\",\n",
    "    yaxis_title=\"Metric value\",\n",
    "    legend_title=\"Metric\"\n",
    ")\n",
    "\n",
    "fig.write_image(f\"Plots/Others/CrossValidationMetrics.png\", scale=2)\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = utils.load_model('./results/Models', model_type=\"LightGBM\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pd.read_csv('Data/Secret/Test_secret.csv')\n",
    "df_ = df_secret_ID\n",
    "df_pred = utils.bronze_to_gold(df_, kept_columns=kept_columns)\n",
    "y_pred = model.predict(df_pred.iloc[:,:-1].values)\n",
    "\n",
    "df_ = utils.bronze_to_gold(df_)\n",
    "di = {0: 'A', 1: 'DB', 2: 'I', 3: 'SW', 4: 'SS', 5:'B'}\n",
    "for q in ['FrL', 'FrG', 'Eo']:\n",
    "    df_[f'log({q})'] = np.log10(df_[f'{q}'])\n",
    "\n",
    "df_[\"Predicted\"] = y_pred\n",
    "df_[\"Correct\"] = (df_['Flow_label']==df_['Predicted'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dimensions = list([ \n",
    "                   dict(label='log(FrL)', values=np.log10(df_['FrL'])),\n",
    "                   dict(label='log(FrG)', values=np.log10(df_['FrG'])),\n",
    "                   dict(label='log(Eo)', values=np.log10(df_['Eo'])),\n",
    "                   dict(label='Ang', values=df_['Ang']),\n",
    "                   dict(range=[0,df_['Flow_label'].max()],\n",
    "                       tickvals = list(di.keys()), ticktext =list(di.values()),\n",
    "                       label='Flow Regime', values=df_['Flow_label']),\n",
    "                    dict(range=[0,df_['Flow_label'].max()],\n",
    "                       tickvals = list(di.keys()), ticktext =list(di.values()),\n",
    "                       label='Predicted', values=df_['Predicted']),\n",
    "                   dict(label=\"Correct\", values=df_['Correct'].astype('int'))\n",
    "                  ])\n",
    "\n",
    "fig = go.Figure(data=go.Parcoords(line = dict(color = df_['Flow_label'], \n",
    "                                colorscale = 'RdBu'), dimensions=dimensions))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_ = df_.join(pd.read_csv('Data/Secret/Test_secret.csv'), how=\"left\", lsuffix=\"\", rsuffix=\"Right\")\n",
    "df_ = df_.join(df_secret_ID, how=\"left\", lsuffix=\"\", rsuffix=\"Right\")\n",
    "dimensions = list([ \n",
    "                   dict(label='log(FrL)', values=np.log10(df_['FrL'])),\n",
    "                   dict(label='log(FrG)', values=np.log10(df_['FrG'])),\n",
    "                   dict(label='ID', values=df_['ID']),\n",
    "                   dict(label='Ang', values=df_['Ang']),\n",
    "                   dict(range=[0,df_['Flow_label'].max()],\n",
    "                        tickvals = list(di.keys()), ticktext =list(di.values()),\n",
    "                       label='Flow Regime', values=df_['Flow_label']),\n",
    "                   dict(label=\"Correct\", values=df_['Correct'].astype('int'))\n",
    "                  ])\n",
    "\n",
    "fig = go.Figure(data=go.Parcoords(line = dict(color = df_['Correct'], \n",
    "                                colorscale = [(0.00, \"red\"),   (0.33, \"red\"),\n",
    "                                                     (0.33, \"blue\"), (0.66, \"blue\"),\n",
    "                                                     (0.66, \"green\"),  (1.00, \"green\")]), dimensions=dimensions))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 64-bit ('Phoenix-Ubuntu-1-1': conda)",
   "language": "python",
   "name": "python388jvsc74a57bd04f519d92e96079f09ba8c7572e77a42ab48cf403df497fdccf4a38e5890942fe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
