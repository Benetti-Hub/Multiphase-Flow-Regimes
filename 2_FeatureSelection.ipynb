{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection Methods\n",
    "\n",
    "This notebook is dedicated to performing feature selection using the best model found during autoML. The following approaches are considered:\n",
    "   - VarianceThreshold: will use the variance to determine if a variable is meaningfull\n",
    "   - SelectKBest: will use ANOVA to keep only the most relevant features\n",
    "   - SequentialForwardSelection: will greedily build an estimator from an empty subset of features\n",
    "    \n",
    "The first two methods are filtering methods, meaning that they do not consider hidden interaction between variables. They are fast, but they do not guarantee of generating the best possible model. Sequential Forward Selection on the other hand is a wrapper method, that will consider interaction between the features, but it is very slow when the number of features is significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from script import utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up\n",
    "\n",
    "Load the gold dataframe (the one read to be ingested by the machine learning algorithms, where missing values are removed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Data/Train_gold.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'Flow_label'\n",
    "features = df.columns.drop(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split features from labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df[features], df[target].values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variance Threshold\n",
    "\n",
    "This method will filter out all the features with less variance than the one imposed by the user. Since the features we are dealing with have different scales, this filtering method will only be used to filter out constant features from the dataset. (I mean, this step is useless given the imposed threshold, it will just remove contant values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_filter = VarianceThreshold(threshold=0)\n",
    "_ = constant_filter.fit(X)\n",
    "discarded_columns = X.columns[np.invert(constant_filter.get_support())]\n",
    "discarded_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[X.columns[constant_filter.get_support()]]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X.columns = [r'$\\theta$', r'$Re_{L}$', r'$Re_{G}$', \n",
    "             r'$Fr_{L}$', r'$Fr_{G}$', r'$N_{L}$', \n",
    "             r'$X_{LM}$', r'$X_{LM}^2$', r'$Y$', \n",
    "             r'$We$', r'$Eo$', r'$\\alpha_{L}$', \n",
    "             r'$K_{G}$', r'$T_{TB}$']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Univariate Feature Selection\n",
    "The next statistical test is actually to select the best K perfomring features. This is done using the ANOVA test for the variables in the dataframe. Note that using this type of filter does not guarantee generating a list of the best meaningfull variables, since the interactions are not taken into account. Nonetheless is a good test to perform. For the time being we select 80% of the features, of course this is a choice dicated by a random number I had in mind when creating this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_to_keep = round(0.8*len(X.columns))\n",
    "univariate_model = SelectKBest(score_func=f_classif, k=feat_to_keep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "univariate_model.fit(X, y)\n",
    "print(\"Features Discarded: \\n\", X.columns[np.invert(univariate_model.get_support())])\n",
    "print(\"Features Kept: \\n\", X.columns[univariate_model.get_support()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the Feature Importance\n",
    "Once we have determined the best performing features, the next step is to actually plot their importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = f_classif(X, y)\n",
    "imp_f_classif = pd.DataFrame({'Features': X.columns, 'F_score': model[0]}).sort_values(by='F_score', ascending=False)\n",
    "fig, axes = plt.subplots(figsize=(35,10))  \n",
    "axes.set_title(\"ANOVA F-statistics\",fontsize=30)\n",
    "plt.bar(range(imp_f_classif.shape[0]), imp_f_classif.F_score, align=\"center\")\n",
    "plt.xticks(range(imp_f_classif.shape[0]), imp_f_classif['Features'], rotation='vertical', fontsize=30)\n",
    "plt.yticks(fontsize=30)\n",
    "plt.xlim([-1, imp_f_classif.shape[0]])\n",
    "plt.ylabel('F(Î»)', fontsize=30)\n",
    "plt.xlabel('Features', fontsize=30)\n",
    "plt.savefig(f'Plots/FeatureSelection/ANOVA.png', dpi=fig.dpi, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is clear that the most performing feature is the liquid holdup, followed by information about the phase velocities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step Forward Feature Selection\n",
    "Here we perfrom the step forward feature selection. This technique will use a greedy strategy to build the best performing model. Note that every feature kept will never be discarded in future steps unless the floating method is set to true."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from mlxtend.feature_selection import ExhaustiveFeatureSelector as EFS\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgbm"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "X = X[X.columns[univariate_model.get_support()]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "estimator = lgbm.LGBMClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sfs=SFS(estimator, k_features=8, forward=True, floating=True, scoring='accuracy', verbose=2, cv=5, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sfs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization\n",
    "\n",
    "This section is dedicated to the visualization of the results egnerated by the SFS. Since we are interested in other metrics than the one used for SFS, we actually have to retrain the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_di = sfs.get_metric_dict()\n",
    "l = [list(info_di[x]['feature_names']) for x in info_di.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_info(l, cv=5, balance_classes=True):\n",
    "    '''\n",
    "    Simple function to contruct a dataframe containing\n",
    "    all the necessary info about the metrics: \n",
    "    \n",
    "    Input:\n",
    "        l: list of features\n",
    "        cv: number of cross validation folds\n",
    "        balance classes: weather or not to perform SMOTE\n",
    "    Output:\n",
    "        info_df: a dictionary containing metrics info\n",
    "    \n",
    "    '''\n",
    "    info_df = {}\n",
    "    \n",
    "    cv_info = np.zeros((cv, 2))\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42) #SKF\n",
    "    \n",
    "    y_sfs = df[target].values.ravel()\n",
    "    for i, features in enumerate(l, start=1):        \n",
    "        X_sfs = X[features].values\n",
    "        \n",
    "        for j, (train_idx, valid_idx) in enumerate(skf.split(X_sfs, y_sfs)):\n",
    "                X_train, y_train = X_sfs[train_idx], y_sfs[train_idx]\n",
    "                X_valid, y_valid = X_sfs[valid_idx], y_sfs[valid_idx]\n",
    "                \n",
    "                if balance_classes:\n",
    "                    X_train, y_train = SMOTE().fit_resample(X_train, y_train)\n",
    "                \n",
    "                model = lgbm.LGBMClassifier()\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "                y_pred = model.predict(X_valid)\n",
    "                \n",
    "                cv_info[j, 0] = accuracy_score(y_valid, y_pred)\n",
    "                cv_info[j, 1] = f1_score(y_valid, y_pred, average='macro')\n",
    "                \n",
    "        info_df[i] = {\n",
    "            'feature_names'  : features,            \n",
    "            'mean_acc' : np.mean(cv_info[:, 0]),\n",
    "            'std_acc'  : np.std(cv_info[:, 0]),\n",
    "            'mean_f1'  : np.mean(cv_info[:, 1]),\n",
    "            'std_f1'   : np.std(cv_info[:, 1]),\n",
    "        }         \n",
    "                        \n",
    "    return info_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_di = generate_info(l, balance_classes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.DataFrame.from_dict(info_di).T\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=scores.index, y=scores['mean_acc'],\n",
    "    error_y=dict(type='data', array=3*scores['std_acc']),\n",
    "    name='Accuracy'\n",
    "))\n",
    "\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=scores.index, y=scores['mean_f1'],\n",
    "    error_y=dict(type='data', array=3*scores['std_f1']),\n",
    "    name='F1 score'\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    width=800,\n",
    "    height=600,\n",
    "    title=\"Sequential Floating Forward Selection Results\",\n",
    "    xaxis_title=\"Features used\",\n",
    "    yaxis_title=\"Metric value\",\n",
    "    legend_title=\"Metric\",\n",
    "    #paper_bgcolor='rgb(239,239,239)',\n",
    "    #plot_bgcolor='rgb(255,255,255)'\n",
    ")\n",
    "\n",
    "fig.add_annotation(\n",
    "            x=6,  # arrows' head\n",
    "            y=scores.iloc[5]['mean_acc'],  # arrows' head\n",
    "            ax=6.3,  # arrows' tail\n",
    "            ay=scores.iloc[5]['mean_acc']-0.1,  # arrows' tail\n",
    "            xref='x',\n",
    "            yref='y',\n",
    "            axref='x',\n",
    "            ayref='y',\n",
    "            showarrow=True,\n",
    "            arrowhead=2,\n",
    "            arrowsize=1,\n",
    "            arrowwidth=1,\n",
    "            font=dict(\n",
    "                size=20,\n",
    "            ),\n",
    "            text=r\"$\\theta, Re_{L}, Fr_{G}, Fr_{L}, X_{LM}, Eo$\",\n",
    "            #bordercolor=\"#ff7f0e\",\n",
    "            #borderwidth=2,\n",
    "            #borderpad=4,\n",
    "            #bgcolor=\"#ffffff\",\n",
    ")\n",
    "\n",
    "#fig.update_yaxes(showgrid=True,  gridcolor=\"grey\", linecolor='black', mirror=True)\n",
    "#fig.update_xaxes(showgrid=True,  gridcolor=\"grey\", linecolor='black', mirror=True)\n",
    "\n",
    "\n",
    "#fig.write_image(f\"Plots/FeatureSelection/SFFS.png\", scale=2)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
